---
layout: post
title: PyTorch入门之60分钟入门闪击战之神经网络
tags: [pytorch, 深度学习]
mermaid: false
math: false
---  

来源于[这里](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)。

神经网络可以使用`torch.nn`包构建。  

现在你对`autograd`已经有了初步的了解，`nn`依赖于`autograd`定义模型并区分它们。一个`nn.Module`包含了层(layers)，和一个用来返回`output`的方法`forward(input)`。  

以下面这个区分数字图像的网络为例：  

<div align="center">
  <img src="../img/2023-05-20/02/classifie_digit.png" alt="classifie_digit">
</div>
  

上图是一个简单的前馈网络。它接受输入，一个层接一层地通过几层网络，最后给出输出。  

典型的神经网络训练程序如下：  

* 定义具有一些可学习参数（或权重）的神经网络
* 迭代输入的数据集
* 通过网络处理输入
* 计算损失（离正确有多远）
* 将梯度回传给网络参数
* 更新网络权重，最典型的更新规则：`weight = weight - learning_rate * gradient`

## 定义网络  

首先，我们需要定义网络：  

```python  
import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net,self).__init__()

        # 1个图形输入通道，6个输出通道，3x3 卷积核
        self.conv1 = nn.Conv2d(1, 6, 3)
        self.conv2 = nn.Conv2d(6, 16, 3)

        # 操作： y = Wx + b
        self.fc1 = nn.Linear(16 * 6 * 6, 120) # 6*6 图像感受野
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84,10)

    def forward(self,x):
        # 最大池化窗口（2, 2）
        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))
        # 如果尺寸是正方形，则只需设置一个数字
        x = F.max_pool2d(F.relu(self.conv2(x)),2)
        x = x.view(-1,self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)

        return x  

    def num_flat_features(self,x):
        size = x.size()[1:]
        num_features = 1
        for s in size:
            num_features *= s
        
        return num_features

net = Net()
print(net)
```

输出：  

```python
Net(
  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
  (fc1): Linear(in_features=576, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
```  

你只需要定义`forward`函数即可，`backward`函数（计算梯度）在你使用`autograd`时自动定义。你可以在`forward`函数中使用任意的Tensor操作。  

模型的可学习参数通过`net.parameters()`返回：  

```python
params = list(net.parameters())
print(len(params))
print(params[0].size())     # conv1层的权重
```

输出：  

```python
10
torch.Size([6, 1, 3, 3])
```  

现在试一下32x32的随机输入。注意：此网络期望的输入尺寸为32x32。要在MNIST数据集上使用此网络，需要现将图形尺寸设为32x32。

```python
input = torch.randn(1,1,32,32)
out = net(input)
print(out)
```

输出：  

```python
tensor([[ 0.0246,  0.0667, -0.0183, -0.0321, -0.0198, -0.0242, -0.0004,  0.0360,
          0.0852, -0.0699]], grad_fn=<AddmmBackward>)
```

零化所有参数的梯度缓存并反向传播随机梯度：  

```python
net.zero_grad()
out.backward(torch.randn(1,10))
```  

> 注意
> `torch.nn`只支持迷你批次。整个`torch.nn`包只支持小批量的样本输入，不支持单个样本。
> 例如，`nn.Conv2d`采用4维张量输入：`nSamples x nChannels x Height x Width`。
> 如果你只有一个样本，那么就需要使用`input.unsqueeze(0)`来添加一个假的批次维度。  

在进行接下来的工作之前，我们梳理下目前接触到所有的类。  

### 梳理  

* `torch.Tensor` - 支持自动梯度操作（例如`backward()`）的多维数组。也存储张量的梯度。
* `nn.Module` - 神经网络模块。便捷的参数封装方式，为模型移动到GPU、导出、导入等提供帮助。  
* `nn.Parameter` - 一种张量，当被指定为模型属性时，自动注册为参数。
* `autograd.Function` - 一种自动梯度操作正向和反向定义的实现。每个张量操作至少创建一个`Function`节点，包含创建张量的函数和编码它的历史记录的函数。   

### 此时，我们做了：  

* 定义了一个神经网络
* 处理了输入值，并调用了反向传播  

### 还剩下：  

* 计算损失
* 更新网络的权重  

## 损失函数（Loss Function）  

损失函数将（输出(output)，目标(target)）作为输入，计算出预估输出与目标之间的距离。  

nn包中包含了几种不同的损失函数。`nn.MSELoss`函数，一种简单的损失函数，计算输入与目标之间的均方差。  

例如：  

```python
output = net(input)
target = torch.randn(10)
target = target.view(1,-1)
criterion = nn.MSELoss()

loss = criterion(output,target)
print(loss)
```

输出：  

```python
tensor(0.8390, grad_fn=<MseLossBackward>)
```

此时，你如果按照`loss`反向使用它的`.grad_fn`属性，你会看到如下的计算图：  
```python
input -> conv2d -> relu -> maxpool2d -> conv2d ->relu -> maxpool2d
      -> view -> linear -> relu -> linear -> relu -> linear
      -> MSELoss
      -> loss
```  

所以，当我们调用`loss.backward()`，整个图中与损失相关的张量开始被微分，图中所有有`requires_grad=True`的张量都将随着梯度累积它们的`.grad`张。  
为了验证这一点，我们回退几步：  

```python
print(loss.grad_fn)  # MSELoss
print(loss.grad_fn.next_functions[0][0])  # Linear
print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU
```

输出：  

```python
<MseLossBackward object at 0x11f40fdd8>
<AddmmBackward object at 0x11f40fe80>
<AccumulateGrad object at 0x11f40fe80>
```

## 反向传递（Backprop）  

为了反向传播误差，我们必须要做的就是调用`loss.backward()`。不过你需要先清除现有的梯度，否则梯度将累积到已有的梯度上。  

现在我们调用`loss.backward()`，查看先conv1层偏置反向传播前后的梯度。  

```python
net.zero_grad()
print('conv1.bias.grad before backward')
print(net.conv1.bias.grad)

loss.backward()

print('conv1.bias.grad after backward')
print(net.conv1.bias.grad)
```

输出： 

```python
conv1.bias.grad before backward
None
conv1.bias.grad after backward
tensor([ 0.0055, -0.0027, -0.0131,  0.0017, -0.0009,  0.0013])
```  

现在，我们知道了如何使用损失函数。  

### 进阶阅读  

神经网络包包含各种模块和损失函数，构成了深度神经网络的构建模块。[这里](https://pytorch.org/docs/stable/nn.html)有完整的列表和文档。  

### 现在未学习的就只剩下：  

* 更新网络的权重  

## 更新权重（Update the weights）  

在实践中使用的最简单的更新规则就是随机梯度下降(Stochastic Gradient Descent, SGD)：  

> weight = weight - learning_rate * gradient

我们可以用简单的Python代码实现上述规则：  

```python
learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
```  

然而，在使用神经网络是，你可能会用到各种各样不同的更新规则，例如 SGD、Nesterov-SGD、Adam、RMSProp等等。为了满足上述要求，PyTorch构建了`torch.optim`包，其中实现了上述方法。使用时非常简单：  

```python
import torch.optim as optim

# 选择你想用的更新规则
optimizer = optim.SGD(net.parameters(),lr=0.01)

# 以下代码写在训练环节中  
optimizer.zero_grad()
output = net(input)
loss = criterion(output,target)
loss.backward()
optimizer.step()
```

> 注意：
> 训练时需要手动调用`optimizer.zero_grad()`来将梯度缓冲区置0。因为梯度是按照**Backprop**部分说明的方式累积的。  

---

<div align="center">
  <img src="../img/qrcode_wechat.jpg" alt="孟斯特">
</div>

> 声明：本作品采用[署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)进行许可，使用时请注明出处。  
> Author: [mengbin](mengbin1992@outlook.com)  
> blog: [mengbin](https://mengbin.top)  
> Github: [mengbin92](https://mengbin92.github.io/)  
> cnblogs: [恋水无意](https://www.cnblogs.com/lianshuiwuyi/)  

---
